{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "721261f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#起心動念：探索文本中的語意情感並進行重點摘要與統整。\n",
    "#場景應用：公關部門或是教育訓練單位，可以透過民眾或學員的文字反饋，了解文字中所透漏的情緒與在乎的重點。\n",
    "#如果是以公關角色來看待，這將有助於快速分辨現行民眾對於公司的正向或負向程度，並從中擷取重點事件。\n",
    "#教育訓練人員則可以透過此方法，了解受訓學員整體回饋是否與滿意度回饋相同，並知道學員最在乎的要點有哪些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cfa9905",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textsum in c:\\users\\123\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (0.28.0)\n",
      "Requirement already satisfied: clean-text in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (0.6.0)\n",
      "Requirement already satisfied: fire in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (0.6.0)\n",
      "Requirement already satisfied: natsort in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (8.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (3.8.1)\n",
      "Requirement already satisfied: torch in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (2.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (4.65.0)\n",
      "Requirement already satisfied: transformers>=4.26.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from textsum) (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\123\\anaconda3\\lib\\site-packages (from tqdm->textsum) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\123\\anaconda3\\lib\\site-packages (from accelerate->textsum) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch->textsum) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch->textsum) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch->textsum) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch->textsum) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch->textsum) (2024.2.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from clean-text->textsum) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from clean-text->textsum) (6.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\123\\anaconda3\\lib\\site-packages (from fire->textsum) (1.16.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\123\\anaconda3\\lib\\site-packages (from fire->textsum) (2.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\123\\anaconda3\\lib\\site-packages (from nltk->textsum) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\123\\anaconda3\\lib\\site-packages (from nltk->textsum) (1.2.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\123\\anaconda3\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text->textsum) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from jinja2->torch->textsum) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\123\\anaconda3\\lib\\site-packages (from sympy->torch->textsum) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#hugging face:https://reurl.cc/xLnXee\n",
    "!pip install textsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a5846da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2024 01:59:15 INFO Loaded model pszemraj/led-large-book-summary to cpu\n"
     ]
    }
   ],
   "source": [
    "#hugging face:https://reurl.cc/xLnXee\n",
    "from textsum.summarize import Summarizer\n",
    "\n",
    "model_name = \"pszemraj/led-large-book-summary\"\n",
    "summarizer = Summarizer(\n",
    "    model_name_or_path=model_name,  # you can use any Seq2Seq model on the Hub\n",
    "    token_batch_length=4096,  # tokens to batch summarize at a time, up to 16384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9869f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3d792c276f4bf0aa751a707f4656c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: Henry's canon of work reflects his wide-ranging experiences and is distinguished for its wit, cleverness, and ironic twist endings. Henry is remembered by his readers as a man who lived a life of adventure and humor\n"
     ]
    }
   ],
   "source": [
    "#type1:手動輸入希望總結的文字\n",
    "#hugging face:https://reurl.cc/xLnXee\n",
    "long_string = \"Henry's rich canon of work reflected his wide-range of experiences and is distinctive for its witticism, clever wordplay, and unexpected twist endings.\"\n",
    "out_str = summarizer.summarize_string(long_string)\n",
    "print(f\"summary: {out_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "844f19f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030ff8a5353844aea2bf23e889b246d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: William Sydney Porter known as O. Henry was an American poet and short story author. His published work reflects his wide-ranging experiences and is distinguished for its wit, wittiness, cleverness, and unusual twist endings\n"
     ]
    }
   ],
   "source": [
    "#type2:節錄本地文件\n",
    "#hugging face:https://reurl.cc/xLnXee\n",
    "file_path = \"C:\\\\Users\\\\123\\\\Desktop\\\\net_learning\\\\hk3.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 將文本拆分成句子\n",
    "sentences = text.split(\".\")\n",
    "# 提取前幾個句子作為摘要\n",
    "summary_sentences = sentences[:3]\n",
    "# 將摘要句子組合成摘要文本\n",
    "summary = \". \".join(summary_sentences)\n",
    "\n",
    "out_str = summarizer.summarize_string(summary)\n",
    "print(f\"summary: {out_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3191c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9977118968963623}, {'label': 'POSITIVE', 'score': 0.9908888339996338}, {'label': 'POSITIVE', 'score': 0.9998646974563599}]\n"
     ]
    }
   ],
   "source": [
    "#將文本內容進行情意分析\n",
    "#hugging face:https://huggingface.co/xai-org/grok-1\n",
    "from transformers import pipeline\n",
    "\n",
    "# 通过pipeline函数加载文本分类模型\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# 执行文本分类\n",
    "result = classifier(summary_sentences)\n",
    "\n",
    "# 输出分类结果\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7247b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model flair/ner-english-ontonotes-large is currently loading', 'estimated_time': 89.60388946533203}\n"
     ]
    }
   ],
   "source": [
    "#將內容進行標籤分類（手動輸入文字）\n",
    "#huggingface:https://huggingface.co/flair/ner-english-ontonotes-large?text=On+September+3nd+George+won+1+dollar+while+watching+Game+of+Thrones.\n",
    "import requests\n",
    "\n",
    "API_URL = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"On September 3nd George won 1 dollar while watching Game of Thrones.\",\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "580c4544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model flair/ner-english-ontonotes-large is currently loading', 'estimated_time': 89.60388946533203}\n"
     ]
    }
   ],
   "source": [
    "#將內容進行標籤分類（擷取電腦檔案）\n",
    "#huggingface:https://huggingface.co/flair/ner-english-ontonotes-large?text=On+September+3nd+George+won+1+dollar+while+watching+Game+of+Thrones.\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/flair/ner-english-ontonotes-large\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# 读取本地文件，提取摘要文本\n",
    "file_path = \"C:\\\\Users\\\\123\\\\Desktop\\\\net_learning\\\\hk3.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# 将文本拆分成句子\n",
    "sentences = text.split(\".\")\n",
    "# 提取前几个句子作为摘要\n",
    "summary_sentences = sentences[:3]\n",
    "# 将摘要句子组合成摘要文本\n",
    "summary = \". \".join(summary_sentences)\n",
    "\n",
    "# 将摘要文本放入 output 中\n",
    "output = query({\"inputs\": summary})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HK4串接RAG（來源：https://github.com/pecu/LLM-RAG-Gradio/blob/main/RAG-Student-Success.ipynb）\n",
    "#思考：透過RAG判斷出文本內容\n",
    "#目的:因為內容範圍小且明確，可以先使用該語法就能符合需求\n",
    "#使用場景:\n",
    "#1.先前將文本進行彙整，並且執行了情意分析、貼標等動作。(HK3)\n",
    "#2.但在這過程中，皆為AI model運算之結果。\n",
    "#3.在實際的應用場域中，user可能會希望針對自己想確認的資訊進行檢索，故串聯該RAG。\n",
    "#如:AI_summary，說他是一個經驗豐富的演說家，但並未揭露有幾年的演說經驗。故此法將針對想進一步了解的資訊進行搜尋。\n",
    "#4.協助user不論在要重新編修summary或添加自己看法時，都能快速校對與查閱資料。(並註記資料頁數，協助查閱完整前後文)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2eff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U pypdf faiss-cpu \n",
    "! pip install -q -U InstructorEmbedding \n",
    "! pip install huggingface_hub -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f8e7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans in c:\\users\\123\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Collecting httpx==0.13.3 (from googletrans)\n",
      "  Obtaining dependency information for httpx==0.13.3 from https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (2024.3.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
      "  Obtaining dependency information for httpcore==0.9.* from https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
      "  Obtaining dependency information for h11<0.10,>=0.8 from https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\123\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
      "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed h11-0.9.0 httpcore-0.9.1 httpx-0.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 4.19.2 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "gradio-client 0.10.1 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b411e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.27.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165fd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.2 in c:\\users\\123\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (0.0.18)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.14 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (0.1.19)\n",
      "Requirement already satisfied: langsmith<0.0.84,>=0.0.83 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain==0.1.2) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.2) (3.21.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.2) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\123\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.2) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.14->langchain==0.1.2) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.14->langchain==0.1.2) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.2) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.2) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.2) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.2) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.2) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain==0.1.2) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: sentence_transformers==2.2.2 in c:\\users\\123\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (4.32.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (2.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (0.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from sentence_transformers==2.2.2) (0.21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\123\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\123\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\123\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\123\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\123\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers==2.2.2) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\123\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers==2.2.2) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\123\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers==2.2.2) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers==2.2.2) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\123\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\123\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\123\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers==2.2.2) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain==0.1.2 \n",
    "! pip install sentence_transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eccafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8517e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import textwrap\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae36167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8598795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dbbe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e7bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec86fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e88d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c32907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69676f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02c777bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19e0899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4466e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be5b938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "    temperature = 0.5\n",
    "    top_p = 0.95\n",
    "    repetition_penalty = 1.15\n",
    "    do_sample = True\n",
    "    max_new_tokens = 400\n",
    "    num_return_sequences=1\n",
    "\n",
    "    split_chunk_size = 800\n",
    "    split_overlap = 0\n",
    "    \n",
    "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    k = 3\n",
    "    \n",
    "    PDFs_path = 'C:\\\\Users\\\\123\\\\Desktop\\\\net_learning'\n",
    "    Embeddings_path =  './faiss_index_py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74dcbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ef13105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id = CFG.model_name,\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": CFG.max_new_tokens,\n",
    "        \"temperature\": CFG.temperature,\n",
    "        \"top_p\": CFG.top_p,\n",
    "        \"repetition_penalty\": CFG.repetition_penalty,\n",
    "        \"do_sample\": CFG.do_sample,\n",
    "        \"num_return_sequences\": CFG.num_return_sequences\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddc29fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    CFG.PDFs_path,\n",
    "    glob=\"hk3.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62898e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = CFG.split_chunk_size,\n",
    "    chunk_overlap = CFG.split_overlap\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03afc282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name = CFG.embeddings_model_repo,\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = texts, \n",
    "    embedding = embeddings\n",
    ")\n",
    "\n",
    "vectordb.save_local(\"faiss_index_py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b6fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name = CFG.embeddings_model_repo,\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    CFG.Embeddings_path,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc1a03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<s>[INST] \n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Answer the code in python language.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:[/INST]\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"question\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7b485ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_chain = LLMChain(prompt=PROMPT, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "979ddc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1216a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    \n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f07abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    llm_response = qa_chain(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans.strip() + time_elapsed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08142077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_after_inst(input_string):\n",
    "    marker_index = input_string.find(\"[/INST]\")\n",
    "    \n",
    "    if marker_index != -1:\n",
    "        return input_string[marker_index + len(\"[/INST]\"):].strip()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34a486e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\123\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(message, history):\n",
    "    output = str(llm_ans(message))\n",
    "    output = extract_text_after_inst(output)\n",
    "    return output\n",
    "\n",
    "CSS =\"\"\"\n",
    ".contain { display: flex; flex-direction: column; }\n",
    ".gradio-container { height: 500vh !important; }\n",
    "#component-0 { height: 100%; }\n",
    "#chatbot { flex-grow: 1; overflow: auto;}\n",
    "\"\"\"\n",
    "            \n",
    "with gr.Blocks(css=CSS) as demo:\n",
    "    with gr.Row(): \n",
    "        with gr.Column():\n",
    "            chat_interface = gr.ChatInterface(\n",
    "                fn=predict,\n",
    "                title='Open-Source LLM for Python Question Answering'\n",
    "            )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef7a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
